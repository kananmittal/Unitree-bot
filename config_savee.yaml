audio:
  sample_rate: 16000
  n_mfcc: 40
  n_fft: 512
  hop_length: 160
  win_length: 400
  f_min: 20.0
  f_max: 8000.0

augmentation:
  enabled: true
  time_stretch_prob: 0.3
  time_stretch_rate: [0.8, 1.2]
  pitch_shift_prob: 0.3
  pitch_shift_steps: [-2, 2]
  noise_injection_prob: 0.3
  noise_level: [0.001, 0.01]
  mixup_prob: 0.4  # Increased from 0.2 for small dataset
  mixup_alpha: 0.4

model:
  n_mfcc: 40
  num_classes: 7  # angry, disgust, fear, happy, neutral, sad, surprise
  conv_channels: [128, 256]
  lstm_hidden: 128
  lstm_layers: 2
  dropout: 0.3  # Increased from 0.2 for small dataset to prevent overfitting
  use_depthwise: true
  use_temp_scale: true
  use_residual: true
  attention_heads: 4

training:
  epochs: 50  # Increased from 30 to allow more training
  batch_size: 8  # Reduced from 16 to get more batches per epoch (30 batches instead of 15)
  lr: 0.002  # Increased from 0.001 for faster learning
  weight_decay: 0.0001
  warmup_epochs: 5
  patience: 15  # Increased from 8 to give more time for improvement
  min_delta: 0.001  # Increased threshold to avoid stopping on noise
  grad_clip: 1.0
  label_smoothing: 0.15  # Increased from 0.1 for small dataset
  class_weights: true
  scheduler: plateau  # Changed from cosine - better for small datasets (doesn't decay as aggressively)
  scheduler_params:
    cosine:
      T_max: 50
      eta_min: 1.0e-06
    plateau:
      factor: 0.5
      patience: 5
      min_lr: 1.0e-05  # Higher minimum LR
    step:
      step_size: 10
      gamma: 0.5

data:
  train_csv: datasets/processed/savee_train.csv
  val_csv: datasets/processed/savee_val.csv
  test_csv: datasets/processed/savee_test.csv
  cache_dir: feat_cache
  num_workers: 2
  val_split: 0.1
  pin_memory: true
  max_audio_length: 10.0
  min_audio_length: 0.5

system:
  device: auto
  seed: 42
  amp: true
  out_dir: checkpoints_savee
  log_dir: logs
  save_best_only: false
  save_interval: 5
